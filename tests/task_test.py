"""Test Agent creation and execution basic functionality."""

from unittest.mock import MagicMock, patch

from crewai.agent import Agent
from crewai.task import Task


def test_task_tool_reflect_agent_tools():
    """Test the functionality of the task tool reflecting agent tools.

    This function sets up a test scenario for an agent that is designed to
    perform research and analysis on content related to AI and AI agents. It
    creates a fake tool and assigns it to a researcher agent with a specific
    role, goal, and backstory. The agent is then tasked with generating a
    list of interesting ideas for an article. The test asserts that the
    tools associated with the task match the expected fake tool.
    """

    from langchain.tools import tool

    @tool
    def fake_tool() -> None:
        "Fake tool"

    researcher = Agent(
        role="Researcher",
        goal="Make the best research and analysis on content about AI and AI agents",
        backstory="You're an expert researcher, specialized in technology, software engineering, AI and startups. You work as a freelancer and is now working on doing research and analysis for a new customer.",
        tools=[fake_tool],
        allow_delegation=False,
    )

    task = Task(
        description="Give me a list of 5 interesting ideas to explore for na article, what makes them unique and interesting.",
        agent=researcher,
    )

    assert task.tools == [fake_tool]


def test_task_tool_takes_precedence_over_agent_tools():
    """Test that task tools take precedence over agent tools.

    This function sets up a test scenario where a researcher agent is
    created with a specific goal and backstory. It defines two tools: a fake
    tool and a fake task tool. The test then creates a task that is assigned
    to the researcher agent, specifying the task's description and the tools
    available for the task. The assertion checks that the tools associated
    with the task are the fake task tool, demonstrating that task tools take
    precedence over the tools defined in the agent.
    """

    from langchain.tools import tool

    @tool
    def fake_tool() -> None:
        "Fake tool"

    @tool
    def fake_task_tool() -> None:
        "Fake tool"

    researcher = Agent(
        role="Researcher",
        goal="Make the best research and analysis on content about AI and AI agents",
        backstory="You're an expert researcher, specialized in technology, software engineering, AI and startups. You work as a freelancer and is now working on doing research and analysis for a new customer.",
        tools=[fake_tool],
        allow_delegation=False,
    )

    task = Task(
        description="Give me a list of 5 interesting ideas to explore for an article, what makes them unique and interesting.",
        agent=researcher,
        tools=[fake_task_tool],
    )

    assert task.tools == [fake_task_tool]


def test_task_prompt_includes_expected_output():
    """Test the task prompt includes the expected output.

    This function sets up a test case for the `Task` class, specifically
    verifying that the task prompt generated by the `Agent` class includes
    the expected output. It creates an instance of `Agent` with a specific
    role and goal, and then creates a `Task` with a description and expected
    output. The test uses mocking to simulate the execution of the task and
    asserts that the `execute_task` method is called with the correct
    parameters.
    """

    researcher = Agent(
        role="Researcher",
        goal="Make the best research and analysis on content about AI and AI agents",
        backstory="You're an expert researcher, specialized in technology, software engineering, AI and startups. You work as a freelancer and is now working on doing research and analysis for a new customer.",
        allow_delegation=False,
    )

    task = Task(
        description="Give me a list of 5 interesting ideas to explore for na article, what makes them unique and interesting.",
        expected_output="Bullet point list of 5 interesting ideas.",
        agent=researcher,
    )

    with patch.object(Agent, "execute_task") as execute:
        execute.return_value = "ok"
        task.execute()
        execute.assert_called_once_with(task=task._prompt(), context=None, tools=[])


def test_task_callback():
    """Test the task execution callback for the Agent class.

    This function sets up a test scenario for the Agent's task execution. It
    creates a Researcher agent with specific attributes and a task that
    requires generating a list of interesting ideas for an article. The test
    mocks the task completion callback to verify that it is called with the
    expected output after executing the task.  The function uses the
    unittest.mock library to patch the execute_task method of the Agent
    class, allowing for controlled testing of the task execution process
    without requiring the actual implementation of the method.
    """

    researcher = Agent(
        role="Researcher",
        goal="Make the best research and analysis on content about AI and AI agents",
        backstory="You're an expert researcher, specialized in technology, software engineering, AI and startups. You work as a freelancer and is now working on doing research and analysis for a new customer.",
        allow_delegation=False,
    )

    task_completed = MagicMock(return_value="done")

    task = Task(
        description="Give me a list of 5 interesting ideas to explore for na article, what makes them unique and interesting.",
        expected_output="Bullet point list of 5 interesting ideas.",
        agent=researcher,
        callback=task_completed,
    )

    with patch.object(Agent, "execute_task") as execute:
        execute.return_value = "ok"
        task.execute()
        task_completed.assert_called_once_with(task.output)


def test_execute_with_agent():
    """Test the execution of a task with an agent.

    This function sets up a researcher agent and a task that requires the
    agent to generate a list of interesting ideas for an article. It uses
    mocking to verify that the agent's `execute_task` method is called with
    the correct parameters when the task is executed.
    """

    researcher = Agent(
        role="Researcher",
        goal="Make the best research and analysis on content about AI and AI agents",
        backstory="You're an expert researcher, specialized in technology, software engineering, AI and startups. You work as a freelancer and is now working on doing research and analysis for a new customer.",
        allow_delegation=False,
    )

    task = Task(
        description="Give me a list of 5 interesting ideas to explore for na article, what makes them unique and interesting.",
        expected_output="Bullet point list of 5 interesting ideas.",
    )

    with patch.object(Agent, "execute_task", return_value="ok") as execute:
        task.execute(agent=researcher)
        execute.assert_called_once_with(task=task._prompt(), context=None, tools=[])
